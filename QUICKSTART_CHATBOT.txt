# Chatbot LLM Upgrade - Quick Start Guide

## What You're Building
Transform your rule-based chatbot into an AI-powered financial assistant using Claude/GPT that can:
- Answer ANY financial question in natural language
- Search transactions ("How much did I spend at Starbucks?")
- Provide personalized advice based on real user data
- Have multi-turn conversations with context memory
- Give explainable, data-driven recommendations

## Where to Start

### Step 1: Read the Documents
1. **PRD_CHATBOT_LLM_UPGRADE.txt** - Full product requirements (30 min read)
2. **TASKS_CHATBOT_LLM_UPGRADE.txt** - Detailed task breakdown with code samples (60 min read)
3. This quickstart guide (5 min)

### Step 2: Set Up Development Environment

#### Get API Keys
1. **Anthropic Claude** (Primary):
   - Sign up at https://console.anthropic.com/
   - Create API key
   - Add to `.env`: `ANTHROPIC_API_KEY=sk-ant-...`

2. **OpenAI** (Backup/Optional):
   - Sign up at https://platform.openai.com/
   - Create API key
   - Add to `.env`: `OPENAI_API_KEY=sk-...`

#### Install Dependencies
```bash
# Backend
cd backend
pip install anthropic>=0.8.0
pip install openai>=1.0.0  # optional
pip install tiktoken  # for token counting

# Frontend
cd frontend
npm install react-markdown  # for markdown rendering
```

### Step 3: Database Setup (2-3 hours)

**Tasks: TASK-1.1, TASK-1.2, TASK-1.3**

Create migration file:
```bash
cd backend
alembic revision -m "create_chat_tables"
```

Add this to the migration:
```python
def upgrade():
    # chat_messages table
    op.create_table(
        'chat_messages',
        sa.Column('message_id', sa.Integer, primary_key=True),
        sa.Column('user_id', sa.String(50), sa.ForeignKey('users.user_id'), nullable=False),
        sa.Column('conversation_id', sa.String(36), nullable=False),
        sa.Column('role', sa.String(20), nullable=False),
        sa.Column('content', sa.Text, nullable=False),
        sa.Column('tokens_used', sa.Integer),
        sa.Column('response_time_ms', sa.Integer),
        sa.Column('model_used', sa.String(100)),
        sa.Column('created_at', sa.DateTime, default=datetime.utcnow),
        sa.CheckConstraint("role IN ('user', 'assistant', 'system')", name='valid_role')
    )

    # Indexes for performance
    op.create_index('idx_user_conversation', 'chat_messages', ['user_id', 'conversation_id'])
    op.create_index('idx_created_at', 'chat_messages', ['created_at'])

    # chat_feedback table
    op.create_table(
        'chat_feedback',
        sa.Column('feedback_id', sa.Integer, primary_key=True),
        sa.Column('message_id', sa.Integer, sa.ForeignKey('chat_messages.message_id'), nullable=False),
        sa.Column('user_id', sa.String(50), sa.ForeignKey('users.user_id'), nullable=False),
        sa.Column('rating', sa.Integer, nullable=False),
        sa.Column('feedback_text', sa.Text),
        sa.Column('created_at', sa.DateTime, default=datetime.utcnow),
        sa.CheckConstraint('rating BETWEEN 1 AND 5', name='valid_rating')
    )

def downgrade():
    op.drop_table('chat_feedback')
    op.drop_table('chat_messages')
```

Run migration:
```bash
alembic upgrade head
```

Create models in `backend/app/models/chat.py` (see TASKS doc for full code)

### Step 4: Build Backend API (Day 1-2)

#### Quick Implementation Order:

**Day 1 Morning: LLM Integration (4-5 hours)**
- TASK-2.1: Set up Anthropic SDK
- TASK-2.2: Create LLM service abstraction
- Test with simple "Hello" message

**Day 1 Afternoon: Context Building (4 hours)**
- TASK-2.3: Create ContextBuilder service
- TASK-2.4: Design system prompt
- Test context generation for sample user

**Day 2 Morning: Chat Endpoint (4 hours)**
- TASK-3.1: Create POST /chat/message endpoint
- Test end-to-end backend flow

**Day 2 Afternoon: Supporting Endpoints (3 hours)**
- TASK-3.2: GET /chat/history
- TASK-3.3: DELETE /chat/history
- TASK-3.4: POST /chat/feedback

### Step 5: Update Frontend (Day 3)

**Morning: Core Integration (4 hours)**
- TASK-4.1: Add API methods to lib/api.ts
- TASK-4.2: Update FinancialChatbot.tsx
  - Remove old `generatePersonalizedAdvice()` function (lines 62-202)
  - Replace with API call in `handleSend()`

**Afternoon: Polish (3 hours)**
- TASK-4.3: Add feedback buttons
- TASK-4.4: Improve loading states
- Test in browser

### Step 6: Testing (Day 4)

**Critical Tests**:
- TASK-5.1: Unit test context builder
- TASK-5.2: Unit test LLM service
- TASK-5.3: Integration test chat endpoint
- TASK-5.4: Manual testing with real questions
- TASK-5.5: Performance testing

**Sample Test Questions**:
```
1. "How much did I spend on food this month?"
2. "What's my biggest spending category?"
3. "Why should I pay down my credit card?"
4. "Show me my Target transactions"
5. "Am I on track with my budget?"
```

### Step 7: Documentation (Day 5)

- TASK-6.1: API documentation
- TASK-6.2: Deployment guide
- TASK-6.3: User guide

---

## File Structure After Phase 1

```
MaxFinanceAI/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”‚   â””â”€â”€ chat.py                    [NEW - ChatMessage, ChatFeedback models]
â”‚   â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”‚   â”œâ”€â”€ llm/                       [NEW]
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ base.py                [BaseLLMService abstract class]
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ claude.py              [ClaudeService implementation]
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ openai.py              [OpenAIService stub]
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ factory.py             [get_llm_service()]
â”‚   â”‚   â”‚   â””â”€â”€ chat/                      [NEW]
â”‚   â”‚   â”‚       â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚       â”œâ”€â”€ context_builder.py     [ContextBuilder service]
â”‚   â”‚   â”‚       â””â”€â”€ prompts.py             [System prompts and templates]
â”‚   â”‚   â””â”€â”€ api/
â”‚   â”‚       â””â”€â”€ chat.py                    [NEW - /chat/* endpoints]
â”‚   â”œâ”€â”€ tests/
â”‚   â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”‚   â”œâ”€â”€ test_context_builder.py    [NEW]
â”‚   â”‚   â”‚   â””â”€â”€ test_llm_service.py        [NEW]
â”‚   â”‚   â””â”€â”€ api/
â”‚   â”‚       â””â”€â”€ test_chat.py               [NEW]
â”‚   â”œâ”€â”€ alembic/
â”‚   â”‚   â””â”€â”€ versions/
â”‚   â”‚       â””â”€â”€ XXX_create_chat_tables.py  [NEW]
â”‚   â””â”€â”€ requirements.txt                   [UPDATE - add anthropic]
â”‚
â”œâ”€â”€ frontend/
â”‚   â””â”€â”€ src/
â”‚       â”œâ”€â”€ lib/
â”‚       â”‚   â””â”€â”€ api.ts                     [UPDATE - add chat methods]
â”‚       â”œâ”€â”€ types/
â”‚       â”‚   â””â”€â”€ chat.ts                    [NEW - ChatMessage, ChatResponse types]
â”‚       â””â”€â”€ components/
â”‚           â””â”€â”€ user/
â”‚               â””â”€â”€ FinancialChatbot.tsx   [UPDATE - use API instead of rules]
â”‚
â”œâ”€â”€ PRD_CHATBOT_LLM_UPGRADE.txt           [NEW - This PRD]
â”œâ”€â”€ TASKS_CHATBOT_LLM_UPGRADE.txt         [NEW - Task breakdown]
â””â”€â”€ QUICKSTART_CHATBOT.txt                [NEW - This file]
```

---

## Key Code Snippets

### 1. Calling Claude API (LLM Service)
```python
# backend/app/services/llm/claude.py
from anthropic import Anthropic

class ClaudeService:
    def __init__(self, api_key: str):
        self.client = Anthropic(api_key=api_key)

    async def send_message(self, system_prompt, user_message, conversation_history, max_tokens=1024):
        messages = conversation_history + [{"role": "user", "content": user_message}]

        response = self.client.messages.create(
            model="claude-3-5-sonnet-20241022",
            max_tokens=max_tokens,
            system=system_prompt,
            messages=messages
        )

        return {
            "content": response.content[0].text,
            "tokens_used": response.usage.input_tokens + response.usage.output_tokens,
            "model": response.model
        }
```

### 2. Building User Context
```python
# backend/app/services/chat/context_builder.py
class ContextBuilder:
    async def build_context(self, user_id: str) -> str:
        # Fetch data in parallel
        profile = await self._get_profile(user_id)
        transactions = await self._get_recent_transactions(user_id, limit=20)
        signals = await self._get_signals(user_id)
        recommendations = await self._get_recommendations(user_id)

        # Format into text
        context = f"""
USER PROFILE:
- Name: {profile['name']}
- Monthly Income: ${profile.get('income', 0):,.2f}
- Credit Utilization: {self._get_credit_utilization(signals)}%

RECENT TRANSACTIONS (Last 30 Days):
{self._format_transactions(transactions)}

ACTIVE RECOMMENDATIONS:
{self._format_recommendations(recommendations)}
"""
        return context
```

### 3. Chat Endpoint
```python
# backend/app/api/chat.py
@router.post("/message")
async def send_message(request: ChatRequest, db: AsyncSession = Depends(get_db)):
    # 1. Validate user
    user = await get_user(db, request.user_id)
    if not user.consent_status:
        raise HTTPException(403, "User has not provided consent")

    # 2. Build context
    context_builder = ContextBuilder(db)
    context = await context_builder.build_context(request.user_id)

    # 3. Get LLM response
    llm = get_llm_service()
    response = await llm.send_message(
        system_prompt=SYSTEM_PROMPT_V1,
        user_message=f"{context}\n\nUser Question: {request.message}",
        conversation_history=[]
    )

    # 4. Save to database and return
    # ... save user and assistant messages ...
    return ChatResponse(...)
```

### 4. Frontend API Call
```typescript
// frontend/src/components/user/FinancialChatbot.tsx
const handleSend = async () => {
  setLoading(true);

  try {
    const response = await api.chat.sendMessage(userId, input, conversationId);

    setMessages(prev => [...prev, {
      role: 'assistant',
      content: response.response,
      timestamp: new Date(response.timestamp)
    }]);

    setConversationId(response.conversation_id);
  } catch (error) {
    // Show error message
  } finally {
    setLoading(false);
  }
};
```

---

## Expected Results After Phase 1

âœ… **Backend**:
- Working `/chat/message` endpoint
- Claude API integration
- Context includes user profile, transactions, signals, recommendations
- Responses in < 5s (p95)

âœ… **Frontend**:
- Chat UI uses API instead of hardcoded rules
- Can ask ANY question and get intelligent response
- Loading states and error handling work
- Feedback buttons functional

âœ… **Quality**:
- Unit tests passing
- Integration tests passing
- Manual test scenarios validated
- Performance targets met

---

## Phase 1 Timeline (1 Week)

**Monday**: Database + LLM Integration (TASK-1.*, TASK-2.1, TASK-2.2)
**Tuesday**: Context Building + Prompts (TASK-2.3, TASK-2.4)
**Wednesday**: Chat Endpoints (TASK-3.*)
**Thursday**: Frontend Integration (TASK-4.*)
**Friday**: Testing + Documentation (TASK-5.*, TASK-6.*)

**Total**: ~40-50 hours of work

---

## Common Issues & Solutions

### Issue 1: "Anthropic API key not found"
**Solution**:
- Make sure `.env` file exists in `backend/` directory
- Add: `ANTHROPIC_API_KEY=sk-ant-your-key-here`
- Restart backend server

### Issue 2: "Context exceeds token limit"
**Solution**:
- Reduce transaction limit from 20 to 10
- Summarize older signals instead of full details
- Use token counting to truncate oldest data

### Issue 3: "Responses are too slow (>5s)"
**Solution**:
- Check LLM API response time in logs
- Reduce max_tokens from 1024 to 512
- Implement caching for user context (1 min TTL)
- Use asyncio.gather() for parallel data fetching

### Issue 4: "Frontend not updating after API call"
**Solution**:
- Check browser console for CORS errors
- Verify API endpoint URL is correct
- Check that conversationId is being stored in state
- Ensure messages array is being updated correctly

---

## What's Next?

After Phase 1 is complete and stable:

**Phase 2 (Week 2)**: Advanced Features
- Multi-turn conversation memory
- Transaction search with NL queries
- Spending comparisons
- Guardrails and safety
- Rate limiting
- OpenAI fallback

**Phase 3 (Week 3)**: UX Enhancements
- Streaming responses (SSE)
- Conversation history UI
- Suggested questions
- Response action buttons
- Markdown formatting

**Phase 4 (Week 4)**: Analytics & Launch
- Analytics dashboard
- Performance optimization
- A/B testing framework
- Monitoring and alerts
- Beta launch â†’ Full launch

---

## Success Metrics

Track these metrics to validate Phase 1 success:

ðŸ“Š **Engagement**:
- Messages per session: Target 5+ (up from ~2 with rules-based)
- Chat open rate: 60% of users
- Return usage: 40% come back to chat

âš¡ **Performance**:
- Response time p95: < 5s (Phase 1), < 3s (Phase 4)
- Error rate: < 2%
- Uptime: 99.9%

ðŸ˜Š **Quality**:
- User satisfaction (ðŸ‘ rate): > 85%
- Question comprehension: > 95%
- Conversation length: 5+ messages

ðŸ’° **Cost**:
- LLM API cost: < $0.10 per user per month
- Token usage: < 2000 tokens per request average

---

## Need Help?

**Documentation**:
- PRD: Full product requirements and architecture
- TASKS: Step-by-step implementation guide with code
- This Quickstart: Get up and running fast

**Anthropic Claude Docs**:
- https://docs.anthropic.com/claude/reference/messages_post

**Questions?**
- Check TASKS document for code examples
- Review PRD for architecture decisions
- Test with real users early and often

---

**Last Updated**: 2025-11-05
**Version**: 1.0
**Author**: AI Product Team

Good luck! ðŸš€
